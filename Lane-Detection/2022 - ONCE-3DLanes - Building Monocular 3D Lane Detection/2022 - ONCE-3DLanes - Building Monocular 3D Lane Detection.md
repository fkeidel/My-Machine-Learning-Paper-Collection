# ONCE-3DLanes - Building Monocular 3D Lane Detection
[[homepage](https://once-3dlanes.github.io/)]
[[paper](https://arxiv.org/pdf/2205.00301.pdf)]

# Key points
- presents a real-world **3D lane detection dataset ONCE-3DLanes**, consisting of 211K images with labeled 3D lane points
- This dataset is the **largest real-world lane detection dataset** published up to now, containing more complex road scenarios with **various weather conditions**, different **lighting conditions** as well as a **variety of geographical locations**.
- Furthermore, presents an **extrinsic-free, anchor free
lane detection method**, called **SALAD**, regressing the **3D coordinates**
of lanes **directly from image view** without converting the feature map into the bird’s-eye view (BEV).
- does not use inverse perspective mapping (IPM)

# Details
## ONCE-3DLanes dataset
- **based on** the most recent large-scale autonomous driving dataset [**ONCE**](https://once-for-auto-driving.github.io/), which comprises 1 million scenes from 144 driving hours covering different time periods including morning, noon, afternoon and night, various weather conditions including sunny, cloudy and rainy days, as well as a variety of regions including downtown, suburbs, highway, bridges, tunnels, hills and dips.
- The used **data collection pipeline** consists of five steps: ground segmentation, point cloud projection, human labeling/auto labeling, adaptive lanes blending and point cloud recovery.
## SALAD (Spatial-Aware monocular LAne Detection)
- In contrast to previous 3D lane detection algorithms, which project the image to top view and adopt a set of predefined anchors to predict 3D coordinates, this method does not require human-crafted anchors and known extrinsic parameters
![architecture](salad_architecture.svg)
- The **architecture** is inspired by [SMOKE](https://arxiv.org/abs/2002.10111) and comprises **two branches**: a **semantic awareness branch** and a **spatial contextual branch** 
- The **backbone ([SegFormer](https://arxiv.org/abs/2105.15203))** encodes an input image into deep features and the **two branches**, the semantic
awareness branch and the spatial contextual branch decode the features to **get the lane’s spatial information and the segmentation mask**. 
- Then **3D reconstruction** is performed by integrating this information and finally obtain the **3D lane positions** in real-world scene.
- The **semantantic awareness branch** provides **2D lane point proposals**. The 2D lane coordinates are encoded into a [ground-truth segmentation map S_gt](https://arxiv.org/abs/1802.05591). The semantic awareness branch is trained with image and groundtruth pairs. During inference, the foreground lane points are predicted with the binary mask S from an image.
- The **spatial contextual branch** predicts **3D offsets** du, dv refining the lane points u_s, u_v predicted by semantic awareness branch:\
u = u_s + du\
v = v_s + du.\
The spatial-awareness branch also predicts an offset dz to calculate a **depth map**. Each row of the depth map has a predefined shift alpha_r and scale beta_r. The z value in the depth map is then\
z = alpha_r + beta_r * dz.\
The ground-truth depth map for training is generated by projecting 3D lane points of the data set on the image plane.\
A [depth completion](https://arxiv.org/abs/1802.00036) algorithm is used to get a dense groundtruth map D_gt.
- Rearranging the perspective projection equations and utilizing the fixed camera intrinsic parameters and the depth values from the depth map, the **2D lane proposal points are projected back to 3D locations** to reconstruct 3D lanes.
- The **overall loss** is the combined loss for the segmentation branch and the spacial-contextual branch\
L = L_seg + lambda * L_reg
- When shifting and scaling the input images for **data augmentation**, they adapt the 3D groundtruth points accordingly to be consistent.
- To **compare predicted and groundtruth lanes**, they first use **IoU in the top view** (z-x) (see [SCNN](https://arxiv.org/abs/1712.06080)). In a second step, they **compare the height information** by calculating the distance to from 3D groundtruth points to corresponding nearest points on the predicted 3D lanes (**unilateral chamfer distance**)

